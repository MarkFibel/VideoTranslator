Сравнение современных нейросетевых моделей машинного перевода по критериям **качество перевода**, **скорость инференса** и **ресурсоёмкость**.  
Задача — определить оптимальную модель по соотношению *качество / скорость / вычислительные затраты*.

---

## Metrics

**Метрики качества:**  
- **BLEU** — Оценка BLEU принимает уже существующие идеально хорошие переводы как эталонный перевод и сравнивает выходные данные машинного перевода (кандидата) с этим эталоном. 
В конечном счете это сравнение выражается числом от 0 до 1. Чем выше цифра, тем лучше оценка.
- **COMET** — по сути метрика аналогичная BLEU  

**Скорость инференса:**  
Скорость инференса — это время, за которое модель выполняет перевод (или другое предсказание).
Насколько быстро модель работает при использовании (без обучения)

**Ресурсоёмкость:**  
- Оценивается по требуемой **VRAM**, возможности работы на **CPU**, и наличию оптимизаций

---

## Model comparision table

| Модель | Размер / Архитектура | Скорость (1000 предложений, ~30tok) | Качество (BLEU) | COMET | Ресурсоёмкость / Требования |
|---|---|---|---|---|---|
| **Helsinki-NLP / OPUS-MT (opus-mt-en-ru / ru-en)** | ~300M (base Transformer) | ~50–90 s (0.05–0.09 s/пред) | ~22-32% | 0.45–0.60 | **Очень лёгкая**, работает на **CPU в реальном времени**. 1–2 GB RAM. GPU не требуется. Оптимизируется |
| **MarianMT (OPUS / official)** | ~400M | ~40–80 s (0.04–0.08 s/пред) | ~30% | 0.50–0.65 | **Средняя**; хорошо работает на **CPU**, быстрее с GPU. Требует 1.5–3 GB VRAM. Оптимизуема |
| **mBART-50 (large)** | ~610M | ~180–300 s (0.18–0.30 s/пред) | ~26-30% | 0.55–0.70 | **Требует GPU** для нормальной скорости. 4–6 GB VRAM минимум. CPU-инференс возможен, но очень медленный. |
| **M2M-100 (418M / 1.2B)** | 418M (base) / 1.2B (large) | 110–400 s (в зависимости от версии) | ~25-32% | 0.55–0.75 | **GPU рекомендован**. 418M можно на CPU (медленно), large требует ≥8 GB VRAM. |
| **NLLB-200 (600M distilled / 1.3B)** | 600M / 1.3B | 70–260 s (0.07–0.26 s/пред) | ~30-37% | 0.60–0.82 | **Distilled 600M** — отличная на **GPU средней мощности (6–8 GB)**, возможен CPU-инференс (медленный). **1.3B** требует ≥10 GB VRAM. Оптимизируема |
| **T5 (Base / Large, finetuned for MT)** | 220M / 770M | 120–420 s (0.12–0.42 s/пред) | ~27-30% | 0.55–0.72 | Универсальная, но тяжёлая. **GPU желателен** (≥8 GB для Large). На CPU — очень медленно. Оптимизируема |

---


## Recommendations

| Сценарий | Оптимальная модель | Обоснование |
|---|---|---|
| **Максимальное качество перевода** | *NLLB-200 (1.3B)* | Лучшие значения BLEU и COMET, особенно для EN↔RU. Поддержка 200 языков. |
| **Баланс скорость / качество (GPU)** | *NLLB-200 distilled (600M)* | Почти SOTA качество при умеренных требованиях. Хорошо масштабируется. |
| **Быстрый и лёгкий перевод (CPU / edge)** | *Helsinki-NLP / MarianMT (OPUS)* | Можно на CPU, потому что легкая. Единственный плюс, наверное |
| **Универсальные задачи text-to-text (обобщённый T5-пайплайн)** | *T5 finetuned* | Подходит для систем, где перевод — часть мультизадачного пайплайна. |

---

## Optimisation

- Использовать **CTranslate2** для MarianMT / Helsinki → ускорение в 3–6 раз.  
- Применять **quantization (int8 / int4)** для снижения VRAM без заметной потери BLEU.   
- Уменьшение **beam size (1–2)** сокращает время инференса с небольшой потерей качества.  
- Для больших моделей — использовать **mixed precision (fp16)** и **batch inference**.  

---

## Resourses

- [Helsinki-NLP / OPUS-MT Models](https://huggingface.co/Helsinki-NLP)
- [MarianMT / CTranslate2 documentation](https://github.com/OpenNMT/CTranslate2)
- [mBART-50 Paper & Model Card (Facebook AI)](https://huggingface.co/facebook/mbart-large-50)
- [M2M-100 (Meta AI)](https://huggingface.co/facebook/m2m100_418M)
- [NLLB-200 Project (Meta AI)](https://ai.meta.com/research/no-language-left-behind/)
- [T5 Overview (Google Research)](https://huggingface.co/docs/transformers/model_doc/t5)
- [COMET Metric Overview](https://github.com/Unbabel/COMET)

